{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating a new Scrapy project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create scafolding for new project\n",
    "\n",
    "```python\n",
    "\n",
    "scrapy startproject flightRadar\n",
    "\n",
    "# scrapy - key word for scrapy shell command (CLI - command line interface)\n",
    "# startproject - function for creating new project\n",
    "# flighRadar - name of project\n",
    "```\n",
    "\n",
    "Above command create scaffolding:\n",
    "\n",
    "![Scaffolding](image.png)\n",
    "\n",
    "---\n",
    "\n",
    "*** Open Project in PyCharm ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Bash\n",
    "alias pycharm=\"open -a /Applications/PyCharm*.app\"\n",
    "cd to-project\n",
    "pycharm .\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Writing a spider to crawl a site and extract data\n",
    "\n",
    "Spiders are classes that you define and that Scrapy uses to scrape information from a website.\n",
    "\n",
    "Create ***flights_spider.py*** inside ***spiders*** directory.\n",
    "\n",
    "**Rules of spider**\n",
    "1. **name:** identifies the Spider. It must be unique within a project, that is, you canâ€™t set the same name for different Spiders.\n",
    "2. **start_requests():** must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n",
    "3. **parse():** a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of TextResponse that holds the page content and has further helpful methods to handle it.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class FlightsSpider(scrapy.Spider):\n",
    "    name = \"flightsSpider2\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'https://www.ryanair.com/pl/pl/'\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        filename = './flight.html'\n",
    "        with open(filename, 'a') as f:\n",
    "            f.write(str(response.body))\n",
    "        self.log('Saved file %s' % filename)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "*** Run spider ***\n",
    "\n",
    "```python\n",
    "scrapy crawl flightsSpider\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Using spider arguments\n",
    "\n",
    "```python\n",
    "scrapy crawl flightSpider -o fligh.json\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
